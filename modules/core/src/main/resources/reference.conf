# Copyright (c) 2014-present Snowplow Analytics Ltd. All rights reserved.
#
# This software is made available by Snowplow Analytics, Ltd.,
# under the terms of the Snowplow Limited Use License Agreement, Version 1.0
# located at https://docs.snowplow.io/limited-use-license-1.0
# BY INSTALLING, DOWNLOADING, ACCESSING, USING OR DISTRIBUTING ANY PORTION
# OF THE SOFTWARE, YOU AGREE TO THE TERMS OF SUCH LICENSE AGREEMENT.

{

  "license" {
    "accept": "false"
    "accept": ${?ACCEPT_LIMITED_USE_LICENSE}
  }

  "gcpUserAgent": {
    "productName": "Snowplow OSS"
    "productName": ${?GCP_USER_AGENT_PRODUCT_NAME}
    "productVersion": "lake-loader"
  }

  "output": {
    "good": {
      "type": "Delta"
      "type": ${?packaging.output.good.type}

      "deltaTableProperties": {
        "delta.logRetentionDuration": "interval 1 days"
        "delta.dataSkippingStatsColumns": "load_tstamp,collector_tstamp,derived_tstamp,dvce_created_tstamp"
        "delta.checkpointInterval": "50"
      }

      "hudiTableOptions": {
        "hoodie.table.name": "events"
        "hoodie.table.keygenerator.class": "org.apache.hudi.keygen.TimestampBasedKeyGenerator"
        "hoodie.table.partition.fields": "load_tstamp"
        "hoodie.table.precombine.field": "load_tstamp"
        "hoodie.keygen.timebased.timestamp.scalar.time.unit": "microseconds"
        "hoodie.keygen.timebased.output.dateformat": "yyyy-MM-dd"
      }

      "hudiWriteOptions": {
        # -- This loader works most efficiently with BULK_INSERT instead of INSERT
        "hoodie.datasource.write.operation": "BULK_INSERT"

        # -- Record key and partition settings. Chosen to be consistent with `hudiTableOptions`.
        "hoodie.keygen.timebased.timestamp.type": "SCALAR"
        "hoodie.datasource.write.reconcile.schema": "true"
        "hoodie.datasource.write.partitionpath.field": "load_tstamp"
        "hoodie.schema.on.read.enable": "true"
        "hoodie.metadata.index.column.stats.column.list": "load_tstamp,collector_tstamp,derived_tstamp,dvce_created_tstamp"
        "hoodie.metadata.index.column.stats.enable": "true"
        "hoodie.datasource.write.keygenerator.consistent.logical.timestamp.enabled": "true"

        # -- Configures how Hudi manages the timeline
        "hoodie.embed.timeline.server": "true"
        "hoodie.embed.timeline.server.reuse.enabled": "true"
        "hoodie.filesystem.view.incr.timeline.sync.enable": "true"
        "hoodie.filesystem.view.type": "SPILLABLE_DISK"

        # -- Hive sync is disabled by default. But if someone does enable it then these are helpful settings:
        "hoodie.datasource.hive_sync.partition_extractor_class": "org.apache.hudi.hive.MultiPartKeysValueExtractor"
        "hoodie.datasource.hive_sync.partition_fields": "load_tstamp_date"
        "hoodie.datasource.hive_sync.support_timestamp": "true"

        # -- Clustering: Every 4 commits, rewrite the latest parquet files to boost their size.
        "hoodie.clustering.inline": "true"
        "hoodie.clustering.plan.partition.filter.mode": "RECENT_DAYS"
        "hoodie.clustering.plan.strategy.daybased.lookback.partitions": "1"
        "hoodie.clustering.plan.strategy.target.file.max.bytes": "500000000" # 500 MB
        "hoodie.clustering.plan.strategy.small.file.limit": "100000000" # 100 MB
        "hoodie.clustering.inline.max.commits": "4" # Should be smaller than the ratio `target.file.max.bytes` / `small.file.limit`
        "hoodie.clustering.plan.strategy.single.group.clustering.enabled": "false"

        # -- Parallelism. This loader works best when we prevent Hudi from ever running >1 task at a time
        "hoodie.file.listing.parallelism": "1"
        "hoodie.metadata.index.bloom.filter.parallelism": "1"
        "hoodie.metadata.index.column.stats.parallelism": "1"
        "hoodie.clustering.max.parallelism": "1"
        "hoodie.finalize.write.parallelism": "1"
        "hoodie.markers.delete.parallelism": "1"
        "hoodie.rollback.parallelism": "1"
        "hoodie.upsert.shuffle.parallelism": "1"
        "hoodie.bloom.index.parallelism": "1"
        "hoodie.insert.shuffle.parallelism": "1"
        "hoodie.archive.delete.parallelism": "1"
        "hoodie.cleaner.parallelism": "1"
        "hoodie.clustering.plan.strategy.max.num.groups": "1"
      }

      "catalog": {
        "type": "Hadoop"
        "options": {}
      }
    }
  }

  "inMemBatchBytes": 50000000
  "cpuParallelismFraction": 0.75
  "windowing": "5 minutes"
  "numEagerWindows": 1
  "spark": {
    "taskRetries": 3
    "conf": {
      "spark.ui.enabled": "false"
      "spark.local.dir": "/tmp" # This is the default but it's important for us
      "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
      "spark.memory.fraction": "0.3" # Decreased from the Spark default to prevent OOMs
      "spark.sql.parquet.outputTimestampType": "TIMESTAMP_MICROS"
      "spark.sql.parquet.datetimeRebaseModeInWrite": "CORRECTED"
      "spark.memory.storageFraction": "0"
      "spark.databricks.delta.autoCompact.enabled": "false"
    }
    "gcpUserAgent": ${gcpUserAgent}
    "writerParallelismFraction": 0.5
  }

  "skipSchemas": []

  "monitoring": {
    "metrics": {
      "statsd": ${snowplow.defaults.statsd}
      "statsd": {
        "prefix": "snowplow.lakeloader"
      }
    }
    "sentry": {
      "tags": {
      }
    }
    "healthProbe": {
      "port": 8000
      "unhealthyLatency": "1 minute"
    }
  }

  "telemetry": ${snowplow.defaults.telemetry}
}
