{

  # -- Accept the terms of the Snowplow Limited Use License Agreement
  # -- See https://docs.snowplow.io/limited-use-license-1.0/
  "license": {
    "accept": ${?ACCEPT_LIMITED_USE_LICENSE}
  }

  "input": {
    # -- Azure Event Hub name for the source of enriched events (required)
    "topicName": "sp-dev-enriched"

    # -- Azure Event Hub bootstrap servers (required)
    "bootstrapServers": "NAMESPACENAME.servicebus.windows.net:9093"

    # -- Any valid Kafka consumer config options
    # -- This example includes authentication settings for Azure Event Hubs
    consumerConf: {
      "group.id": "snowplow-lake-loader"
      "security.protocol": "SASL_SSL"
      "sasl.mechanism": "PLAIN"
      "sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password="<YOUR_PASSWORD>"
    }
  }

  "output": {

    ## -- DELTA OUTPUT FORMAT -- ##
    "good": {
      # -- URI of the bucket where the data lake will be written (required)
      # -- For Azure blob storage, the uri should start with `abfs://`
      "location": "abfs://snowplow@example.dfs.core.windows.net/events"

      # -- Atomic columns which should be brought to the "left-hand-side" of the events table, to
      # -- enable Delta's Data Skipping feature.
      # -- Default is the following important Snowplow timestamps:
      "dataSkippingColumns": [
        "load_tstamp"
        "collector_tstamp"
        "derived_tstamp"
        "dvce_created_tstamp"
      ]
    }

    ## -- HUDI OUTPUT FORMAT -- ##
#   "good": {
#
#     # -- Tell the loader to use Hudi output format
#     "type": "Hudi"
#
#     # -- URI of the bucket where the data lake will be written (required)
#     # -- For Azure blob storage, the uri should start with `abfs://`
#     "location": "abfs://snowplow@example.dfs.core.windows.net/events"
#
#     # -- Any valid hudi configuration key/value.
#     # -- This can be blank in most setups because the loader already sets sensible defaults.
#     "hudiWriteOptions": {
#       "hoodie.metadata.index.column.stats.column.list": "load_tstamp,collector_tstamp,derived_tstamp,dvce_created_tstamp"
#     }
#
#     # -- Any valid hudi table option
#     "hudiWriteOptions": {
#       "hoodie.keygen.timebased.output.dateformat": "yyyy-MM-dd"
#     }
#   }

    ## -- ICEBERG OUTPUT FORMAT, HADOOP CATALOG -- ##
#   "good": {
#
#     # -- Tell the loader to use Iceberg + Hadoop
#     "type": "IcebergHadoop"
#
#     # -- URI of the bucket where the data lake will be written (required)
#     # -- For Azure blob storage, the uri should start with `abfs://`
#     "location": "abfs://snowplow@example.dfs.core.windows.net/events"
#
#     # -- Name of the database in the hadoop catalog (required)
#     "database": "snowplow"
#
#     # -- Name of the table in the hadoop catalog (required)
#     "table": "events"
#   }

    ## -- ICEBERG OUTPUT FORMAT -- ##
#   "good": {
#
#     # -- Tell the loader to use Iceberg
#     "type": "Iceberg"
#
#     # -- URI of the bucket where the data lake will be written (required)
#     # -- For Azure blob storage, the uri should start with `abfs://`
#     "location": "abfs://snowplow@example.dfs.core.windows.net/events"
#
#     # -- Name of the database in the catalog (required)
#     "database": "snowplow"
#
#     # -- Name of the table in the catalog (required)
#     "table": "events"
#
#     # -- Details of the Iceberg catalog
#     # -- Hadoop is the only supported catalog type on Azure.
#     "catalog": {
#
#       # -- Any valid catalog config option from the Iceberg documentation
#       "options": {
#          # -- For example, to enable the catalog cache
#          "cache-enabled": "true"
#       }
#     }
#   }

    "bad": {
      # -- Azure Event Hub name for the source of enriched events (required)
      "topicName": "sp-dev-bad"

      # -- Azure Event Hub bootstrap servers (required)
      "bootstrapServers": "localhost:9092"

      # -- Any valid Kafka producer config options
      # -- This example includes authentication settings for Azure Event Hubs
      "producerConf": {
        "client.id": "snowplow-lake-loader"
        "security.protocol": "SASL_SSL"
        "sasl.mechanism": "PLAIN"
        "sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password="<YOUR_PASSWORD>"
      }
    }

  }

  # -- Controls how many events are buffered in memory before saving the batch to local disk.
  # -- The default value works well for most reasonably sized VMs.
  "inMemBatchBytes": 25600000

  # -- Controls how the app splits the workload into concurrent batches which can be run in parallel.
  # -- E.g. If there are 4 available processors, and cpuParallelismFraction = 0.75, then we process 3 batches concurrently.
  # -- Adjusting this value can cause the app to use more or less of the available CPU.
  "cpuParallelismFraction": 0.75

  # -- Controls how often we write/commit pending events to the data lake.
  "windowing": "5 minutes"

  # -- Settings relating to the local Spark context use internally by this loader.
  "spark": {

    # -- How many times a Spark task should be retried in case of failure.
    "taskRetries": 3

    # -- Any valid spark configuration key/value.
    # -- This can be blank in most setups because the loader already sets sensible defaults.
    "conf": {
      # -- E.g. to enable the spark ui for debugging:
      "spark.ui.enabled": true
    }

    # -- Controls how many spark tasks run in parallel during writing the events to cloud storage.
    # -- E.g. If there are 8 available processors, and cpuParallelismFraction = 0.25, then we have 2 spark tasks for writing.
    # -- The default value is known to work well. Changing this setting might affect memory usage, file sizes, and/or latency.
    "writerParallelismFraction": 0.25
  }

  # -- Schemas that won't be loaded to the lake.  Optional, default value []
  "skipSchemas": [
    "iglu:com.acme/skipped1/jsonschema/1-0-0"
    "iglu:com.acme/skipped2/jsonschema/1-0-*"
    "iglu:com.acme/skipped3/jsonschema/1-*-*"
    "iglu:com.acme/skipped4/jsonschema/*-*-*"
  ]

  "monitoring": {
    "metrics": {

      # -- Send runtime metrics to a statsd server
      # -- `hostname` is the only required field in order to turn on this feature.
      "statsd": {

        # -- Hostname or IP of a statsd server.
        "hostname": "127.0.0.1"

        # -- Port of the statsd server.
        "port": 8125

        # -- Map of key/value pairs to be send along with the statsd metric.
        "tags": {
          "myTag": "xyz"
        }

        # -- How often to report metrics to statsd.
        "period": "1 minute"

        # -- Prefix used for the metric name when sending to statsd.
        "prefix": "snowplow.lakeloader"
      }
    }

    # -- Report unexpected runtime exceptions to Sentry
    "sentry": {
      "dsn": "https://public@sentry.example.com/1"

      # -- Map of key/value pairs to be included as tags
      "tags": {
        "myTag": "xyz"
      }
    }

    # -- Open a HTTP server that returns OK only if the app is healthy
    "healthProbe": {
      "port": 8000

      # -- Health probe becomes unhealthy if events are not being processed faster than this cuttoff
      # -- time
      "unhealthyLatency": "1 minute"
    }
  }

  # -- Optional, configure telemetry
  # -- All the fields are optional
  "telemetry": {

    # -- Set to true to disable telemetry
    "disable": false

    # -- Identifier intended to tie events together across modules,
    # -- infrastructure and apps when used consistently
    "userProvidedId": "my_company"

  }
}
